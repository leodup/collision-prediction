{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def adjust_video_length(video, desired_frames):\n",
    "    current_frames = video.shape[0]\n",
    "    \n",
    "    if current_frames < desired_frames:\n",
    "        pad_frames = desired_frames - current_frames\n",
    "        padding = torch.zeros((pad_frames, *video.shape[1:]), dtype=video.dtype)\n",
    "        video = torch.cat((video, padding), dim=0)\n",
    "    elif current_frames > desired_frames:\n",
    "        video = video[:desired_frames]\n",
    "    \n",
    "    return video\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def is_image_valid(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.video_dirs = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = os.path.join(self.directory, self.video_dirs[idx])\n",
    "        frames = []\n",
    "        for i in range(22):\n",
    "            image_path = os.path.join(video_dir, f\"image_{i}.png\")\n",
    "            if is_image_valid(image_path):\n",
    "              image = Image.open(image_path).convert(\"RGB\")\n",
    "              frames.append(ToTensor()(image))\n",
    "        video = torch.stack(frames)\n",
    "        # Adjust video length\n",
    "        desired_frames = 22  \n",
    "        video = adjust_video_length(video, desired_frames)\n",
    "        return video\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.video_dirs = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = os.path.join(self.directory, self.video_dirs[idx])\n",
    "        frames = []\n",
    "        for i in range(22):\n",
    "            image_path = os.path.join(video_dir, f\"image_{i}.png\")\n",
    "            if is_image_valid(image_path):\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                frames.append(ToTensor()(image))\n",
    "\n",
    "        mask_path = os.path.join(video_dir, \"mask.npy\")\n",
    "        mask_data = np.load(mask_path)\n",
    "        mask = torch.tensor(mask_data.reshape(22, 160, 240), dtype=torch.long)  # Reshape the mask data\n",
    "        video = torch.stack(frames)\n",
    "        # Adjust video length\n",
    "        desired_frames = 22  \n",
    "        video = adjust_video_length(video, desired_frames)\n",
    "        return video, mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_generated_masks(generated_masks, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for i, mask in enumerate(generated_masks):\n",
    "        output_path = os.path.join(output_directory, f\"mask_{i}.npy\")\n",
    "        np.save(output_path, mask)\n",
    "\n",
    "\n",
    "def calculate_iou_batch(predictions, masks):\n",
    "    smooth = 1e-6\n",
    "    predictions = predictions.argmax(dim=1)\n",
    "    predictions = predictions > 0.5\n",
    "    masks = masks > 0.5\n",
    "\n",
    "    intersection = (predictions & masks).float().sum()\n",
    "    union = (predictions | masks).float().sum()\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m947.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/dks7920/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PretextModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(22, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 22, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = VideoDataset(\"/dataset/unlabeled\")\n",
    "labeled_train_dataset = SegmentationDataset(\"/dataset/train\")\n",
    "labeled_val_dataset = SegmentationDataset(\"/dataset/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "# Self-supervised learning\n",
    "pretext_model = PretextModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(pretext_model.parameters(), lr=0.001)\n",
    "\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n",
    "t = trange(num_epochs)\n",
    "for epoch in t:\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    for batch_idx, videos in enumerate(unlabeled_dataloader):\n",
    "        start_batch = time.time()\n",
    "        videos = videos.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_forward = time.time()\n",
    "        predictions = pretext_model(videos)\n",
    "\n",
    "        start_loss = time.time()\n",
    "        loss = criterion(predictions, videos)  # Modify this line based on the pretext task\n",
    "\n",
    "        start_backward = time.time()\n",
    "        loss.backward()\n",
    "\n",
    "        start_optim = time.time()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time = time.time() - start_batch\n",
    "        t.set_postfix(batch_time=batch_time, batch_idx=batch_idx + 1)\n",
    "\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    t.set_postfix(epoch_time=epoch_time, epoch=epoch + 1, refresh=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27:  90%|█████████ | 27/30 [10:38<01:10, 23.64s/it, epoch_time=23.8, training_loss=0.00191]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "num_samples = 500  # Adjust this to the number of samples you want to use\n",
    "small_unlabeled_dataset = Subset(unlabeled_dataset, range(num_samples))\n",
    "small_unlabeled_dataloader = DataLoader(small_unlabeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "pretext_model = PretextModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(pretext_model.parameters(), lr=0.001)\n",
    "\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "t = trange(num_epochs)\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    for batch_idx, videos in enumerate(small_unlabeled_dataloader):\n",
    "        start_batch = time.time()\n",
    "        videos = videos.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_forward = time.time()\n",
    "        predictions = pretext_model(videos)\n",
    "\n",
    "        start_loss = time.time()\n",
    "        loss = criterion(predictions, videos)  # Modify this line based on the pretext task\n",
    "\n",
    "        start_backward = time.time()\n",
    "        loss.backward()\n",
    "\n",
    "        start_optim = time.time()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time = time.time() - start_batch\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        t.set_postfix(batch_time=batch_time, batch_idx=batch_idx + 1, refresh=False)\n",
    "    \n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(epoch_time=epoch_time, training_loss=avg_train_loss, refresh=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.base_model = pretrained_model.encoder\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(size=(22, 160, 240), mode=\"nearest\"),\n",
    "            nn.Conv3d(32, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(16, 52, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "small_train_dataset = Subset(labeled_train_dataset, range(num_samples))\n",
    "small_val_dataset = Subset(labeled_val_dataset, range(num_samples))\n",
    "train_dataloader = DataLoader(small_train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(small_val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(labeled_train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(labeled_val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised fine-tuning\n",
    "segmentation_model = SegmentationModel(pretrained_model=pretext_model).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(segmentation_model.parameters(), lr=0.001)\n",
    "t = trange(num_epochs, position=0, leave=True)\n",
    "best_iou = 0.1492\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training loop\n",
    "    segmentation_model.train()\n",
    "    for videos, masks in train_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = segmentation_model(videos)\n",
    "        loss = criterion(predictions, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    # Validation loop\n",
    "    segmentation_model.eval()\n",
    "    val_loss = 0\n",
    "    iou_sum = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, masks in val_dataloader:\n",
    "            videos = videos.to(device)\n",
    "            masks = masks.to(device)\n",
    "            predictions = segmentation_model(videos)\n",
    "            loss = criterion(predictions, masks)\n",
    "            val_loss += loss.item()\n",
    "            iou_sum += calculate_iou_batch(predictions, masks)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Calculate average validation loss and IoU\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_iou = iou_sum / num_batches\n",
    "\n",
    "    # Update the progress bar with training and validation information\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(\n",
    "        train_loss=f\"{avg_train_loss:.4f}\",\n",
    "        val_loss=f\"{avg_val_loss:.4f}\",\n",
    "        iou=f\"{avg_iou:.4f}\",\n",
    "        refresh=True,\n",
    "    )\n",
    "    if avg_iou > best_iou:\n",
    "        best_iou = avg_iou\n",
    "        torch.save(segmentation_model.state_dict(), f\"best_model.pth\")\n",
    "        t.write(f\"Best model saved at Epoch {epoch + 1} with IOU: {avg_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_classes(dataset):\n",
    "    unique_classes = set()\n",
    "    for _, mask in dataset:\n",
    "        for frame in mask:  # Iterate through each frame of the mask\n",
    "            classes = np.unique(frame.numpy())\n",
    "            unique_classes.update(classes)\n",
    "    return len(unique_classes)\n",
    "\n",
    "num_classes = count_classes(labeled_train_dataset)\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data2 = np.load(\"/scratch/dks7920/Dataset_Student/train/video_301/mask.npy\")\n",
    "print(f\"Mask data shape: {mask_data2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Implementing a custom pretext task\n",
    "class NextFramePredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextFramePredictionModel, self).__init__()\n",
    "        # Use the same encoder architecture as in the original code\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(22, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        # Use a different decoder architecture for the pretext task\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 3, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replace the pretext model with the new model\n",
    "pretext_model = NextFramePredictionModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "from tqdm import trange\n",
    "\n",
    "t = trange(num_epochs)\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    pretext_model.train()\n",
    "    for videos in small_unlabeled_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        input_videos = videos[:, :-1]\n",
    "        target_videos = videos[:, -1]\n",
    "        predictions = pretext_model(input_videos)\n",
    "        loss = criterion(predictions, target_videos)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_time = time.time() - start_batch\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        t.set_postfix(batch_time=batch_time, batch_idx=batch_idx + 1, refresh=False)\n",
    "    \n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(epoch_time=epoch_time, training_loss=avg_train_loss, refresh=True)\n",
    "\n",
    "# 3. Semi-supervised learning - pseudo-labeling\n",
    "# Generate pseudo-labels for the unlabeled data\n",
    "#unlabeled_dataset = VideoDataset(\"/scratch/dks7920/Dataset_Student/unlabeled\")\n",
    "#unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
    "segmentation_model.eval()\n",
    "pseudo_labels = []\n",
    "with torch.no_grad():\n",
    "    for videos in small_unlabeled_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        predictions = segmentation_model(videos[:, :11]) # Use first 11 frames\n",
    "        pseudo_labels.append(predictions.argmax(dim=1).cpu())\n",
    "        \n",
    "combined_dataset = ConcatDataset([labeled_train_dataset] + [TensorDataset(videos, masks) for videos, masks in zip(unlabeled_dataset, pseudo_labels)])\n",
    "combined_dataloader = DataLoader(combined_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training loop\n",
    "    segmentation_model.train()\n",
    "    for videos, masks in train_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = segmentation_model(videos)\n",
    "        loss = criterion(predictions, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    # Validation loop\n",
    "    segmentation_model.eval()\n",
    "    val_loss = 0\n",
    "    iou_sum = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, masks in val_dataloader:\n",
    "            videos = videos.to(device)\n",
    "            masks = masks.to(device)\n",
    "            predictions = segmentation_model(videos)\n",
    "            loss = criterion(predictions, masks)\n",
    "            val_loss += loss.item()\n",
    "            iou_sum += calculate_iou_batch(predictions, masks)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Calculate average validation loss and IoU\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_iou = iou_sum / num_batches\n",
    "\n",
    "    # Update the progress bar with training and validation information\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(\n",
    "        train_loss=f\"{avg_train_loss:.4f}\",\n",
    "        val_loss=f\"{avg_val_loss:.4f}\",\n",
    "        iou=f\"{avg_iou:.4f}\",\n",
    "        refresh=True,\n",
    "    )\n",
    "    if avg_iou > best_iou:\n",
    "        best_iou = avg_iou\n",
    "        torch.save(segmentation_model.state_dict(), f\"best_model.pth\")\n",
    "        t.write(f\"Best model saved at Epoch {epoch + 1} with IOU: {avg_iou:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
